<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8' />
  <meta http-equiv="X-UA-Compatible" content="chrome=1" />
  <meta name="description" content="Applied Statistics: An Introduction to Statistical Analysis. An overview of basic techniques for sound analysis and interpretation of experimental data for non-statisticians." />
  <meta name="keywords" content="Stats, Statistics, Statistical analysis, Applied statistics">
  <meta name="author" content="Chris Wetherill" />

  <link href='https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&subset=latin' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" media="screen" href="/stylesheets/stylesheet.css">
  <title>Measuring Uncertainty | Applied Statistics</title>
  
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>

<body>
  <div class="content">
    <h1>Measuring Uncertainty</h1>
    <br />
    
    <h2>Measures of Central Tendency and Spread</h2>
    <hr>
    
    <h3>Mean, Median, Mode</h3>
    <p>Mean, median, and mode represent the three common ways of describing important features of a data set at a glance. The <span class="dt">arithmetic mean</span> of a sample, \( x_1, x_2, \ldots, x_n \), is denoted by \( \bar{x} \) (pronounced "x-bar"). For a sample of \( n \) observations, it is calculated:</p>
    
    $$ \bar{x} = \frac{x_1 +x_2+\ldots +x_n}{n} $$
    
    <p>Alternately, it may be represented as:</p>
    
    $$ \bar{x} = \frac{\sum_i^n x_i}{n} $$
    
    <p>This means we add up every item in a set of \( n \) items and then divide by however many items we added together. Here, the capital sigma indicates that we take a sum (that is, we add up all the values). This is the most commonly-used measure of central tendency, although it should never be used when dealing with ordinal or categorical data. (That is, ranked data&mdash;such as "rate these from best to worst"&mdash;or categories&mdash;such as male/female or religion. Although technically possible, the result will be difficult to interpret, if not impossible.)</p>
    
    <p>The <span class="dt">median</span> of a set of data is, quite literally, the middle value. If we take a series of numbers, order them from smallest to largest, and pick the number that's right in the middle (think of a see-saw balancing at its fulcrum), then we have the median. This can be calculated in many of the same cases as the mean and is more resistent to outliers (values that are unusually far away from most other data points) than is the mean.</p>
    
    <p>Finally, the <span class="dt">mode</span> is the most frequently-occurring number in a series of data. Usually, we will compute the mode when working with categorical data: for instance, it could tell us that the majority of survey respondents were female or were Jewish or lived in the Midwest.</p>
    
    <p>If we want to calculate these in R, we would do something like this:</p>
    
    <pre>
      <code>
# First we will generate 200 random values
# with mean 0 and standard deviation 1

> set.seed(0)
> data <- rnorm(200, mean=0, sd=1)

# Now we will find the mean:

> mean(data)
[1] -0.01144155

# We can see that the mean of -0.011 is
# close to the value (0) we specified above.
# Next let's calculate the median

> median(data)
[1] -0.1061751

# We see that the median differs from the mean!
# That means that our data are a little skewed
# We'll talk about skewness in the next section.
      </code>
    </pre>
    <h3>Skewness and Mean vs. Median</h3>
    <p>To get an idea of skewness, let's make a histogram of those data that we just generated. If you aren't familiar with a histogram, it takes your data and counts how many data points fall within a certain range, giving you something like this:</p>
    
    <img src="assets/skewHistogram.jpeg" width="700px" height="366px" />
    
    <p>The smooth line that you see above is the <a href="http://en.wikipedia.org/wiki/Normal_distribution" target="_blank">normal distribution</a>; the columns represent our actual observations. As you can see, the two don't line up perfectly. Rather, our graph is stretched out to the right a bit. We call this stretching <span class="dt">skewness</span>: the idea that, rather than being perfectly symmetrical about the mean, out data are stretched out to the right or to the left. In cases where our data are stretched out to the right, we say that they are skewed right; when they are stretched to the left, we say they are skewed left.</p>
    
    <p>Many times, as in this example, there isn't very much skewness and our data, even though they aren't perfect, are still fairly symmetrical. However, sometimes we will have strong outliers that really mess up that symmetry. For instance:</p>
    
    <img src="assets/skew2.jpeg" width="700px" height="366px" />

    <p>Here, we have a strong rightward skew. Now, let's go ahead and compare the means and medians of our two datasets:</p>
    
    <pre>
      <code>
# First we'll compare the means
> mean(data)
[1] -0.01144155

> mean(data2)
[1] 0.2489018

# And now the medians
> median(data)
[1] -0.1061751

> median(data2)
[1] -0.005767173
      </code>
    </pre>
    
    <p>As you can see, both the mean and the median change. However, the median changes much less than the mean does. (There's a change of 0.26 in the means versus a change of 0.10 in the medians.) In casaes where you have strongly skewed data, it will often be better to describe them using the median rather than the mean: specifically, the median is what we call <span class="dt">resistent to outliers</span>. In other words, when you have a few outliers (numbers that are far away from every other data point), the median will be changed much less than the mean will.</p>
    
    <h3>Standard Deviation</h3>
    <p><span class="dt">Standard deviation</span> (represented as \( s \) or \( \sigma \)) is a measure of dispersion: that is, it tells us how far from or close to the mean our data tend to be. A data set with a small standard deviation, for instance, tells us that most of our data points are fairly close together and are all near the mean. Alternately, a large standard deviation means that our data points are much more spread out. We can visualize this using three data sets, all with mean \( \mu = 0 \) but with different standard deviations:</p>
    
    <img src="assets/stdevs.jpeg" width="700px" height="366px" />
    
    <p>Even though each of these three sets of data has the same mean (0), it's pretty clear that there are some big differences between each of them. This is why standard deviation is important to know and report: without it, you aren't going to be getting a complete picture of what your data look like. They could all be close to the mean, as in the graph on the far left, or they could be much more spread out, as in the graph on the right.</p>
    
    <p>Now, before we can use standard deviations, we have to calculate them. The formula for standard deviation looks like:</p>
    
    $$
    \sigma = \sqrt{\frac{1}{n}\sum_{i=1}^n\left(x_i-\mu\right)^2}\text{, where } \mu=\frac{1}{n}\sum_{i=1}^n(x_i)
    $$
    
    <p>Let's unpack that: it's saying that we start by finding the mean (\( \mu \)) of the data. Next, we'll go ahead and take every data point and subtract the mean from it (the \( x_i-\mu \) part of the equation). So what we're doing is basically finding out how many units away from the mean each data point is. But there's a problem: some data points might give us a negative number, others a positive number. So we'll take that difference and raise it to the second power (remember, a negative number times a negative number is always equal to a positive number). Now we just repeat that for every other data point in our sample and add them all together.</p>
    
    <p>So now we have summed up all of those squared differences, right? Next, we will divide by the number (\( n \)) of observations (just like when we calculated the mean) to get the <span class="dt">average distance from the mean</span>. But there's one last step before we're done: since we raised everything to the second power a couple steps ago, we have to undo that operation. To do this, we'll take the square root of everything, leaving us at last with the standard deviation.</p>
    
    <h3>Variance</h3>
    <p></p>
    
    <h3>Interquartile Range</h3>
    <p></p>
    
    <h2>Sampling Distributions</h2>
    <hr>
    
    <h3>Central Limit Theorem and Sampling Distributions</h3>
    <p></p>
    
    <h3>Standard Error</h3>
    <p></p>
    
    <h2>Visualizing Data</h2>
    <hr>
    
    <h3>Histograms</h3>
    <p></p>
    
    <h3>Kernel Density Plots</h3>
    <p></p>
    
    <h3>Boxplots</h3>
    <p></p>
    
    <h3>Scatterplots</h3>
    <p></p>
    
    <h2>Additional Resources</h2>
    <hr>
    
  </div>
  <div class="nav">
    <ul>
      <li><a href="introToR.html">&larr; Introduction to R</a></li>
      <li><a href="/">Home</a></li>
      <li><a href="design.html">Research Design &rarr;</a></li>
    </ul>
  </div>

</body>
</html>